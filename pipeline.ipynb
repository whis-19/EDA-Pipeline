{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c442aa-5bf5-44e7-aff7-a528a1ad32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from paths import *\n",
    "\n",
    "def convert_json_to_csv(input_folder, output_folder):\n",
    "    messages = []\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        messages.append(f\"Created output folder: {output_folder}\")\n",
    "    \n",
    "    json_files = glob.glob(os.path.join(input_folder, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        messages.append(\"No JSON files found in the folder.\")\n",
    "        return messages\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            records = data.get(\"response\", {}).get(\"data\", [])\n",
    "            df = pd.json_normalize(records)\n",
    "            \n",
    "            base_name = os.path.basename(json_file)\n",
    "            file_name = os.path.splitext(base_name)[0]\n",
    "            output_file = os.path.join(output_folder, file_name + \".csv\")\n",
    "            \n",
    "            df.to_csv(output_file, index=False)\n",
    "            messages.append(f\"Converted {json_file} to {output_file}\")\n",
    "        except Exception as e:\n",
    "            messages.append(f\"Error processing {json_file}: {e}\")\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def merge_csv_files(folder1, folder2, output_file):\n",
    "    messages = []\n",
    "    csv_files1 = glob.glob(os.path.join(folder1, \"*.csv\"))\n",
    "    csv_files2 = glob.glob(os.path.join(folder2, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files1 or not csv_files2:\n",
    "        messages.append(\"No CSV files found in one or both folders.\")\n",
    "        return messages\n",
    "    \n",
    "    dfs1 = [pd.read_csv(file) for file in csv_files1]\n",
    "    dfs2 = [pd.read_csv(file) for file in csv_files2]\n",
    "    \n",
    "    if not dfs1 or not dfs2:\n",
    "        messages.append(\"No data to merge from one or both folders.\")\n",
    "        return messages\n",
    "    \n",
    "    df1 = pd.concat(dfs1, ignore_index=True, sort=False)\n",
    "    df2 = pd.concat(dfs2, ignore_index=True, sort=False)\n",
    "    merged_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        messages.append(f\"Created output directory: {output_dir}\")\n",
    "    \n",
    "    try:\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        messages.append(f\"Merged CSV saved as {output_file}\")\n",
    "    except Exception as e:\n",
    "        messages.append(f\"Error saving merged CSV: {e}\")\n",
    "    \n",
    "    return messages\n",
    "def merger():\n",
    "    st.write(\"Converting JSON to CSV...\")\n",
    "    convert_json_to_csv(json_input_folder, csv_output_folder)\n",
    "\n",
    "    st.write(\"Process completed!\")\n",
    "\n",
    "    st.write(\"Merging CSV files...\")\n",
    "    merge_csv_files(csv_folder1, csv_folder2, merged_csv_output)\n",
    "\n",
    "    st.write(\"Process completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a05270-a038-4df6-8f40-5276907ddf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from paths import *\n",
    "\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\"\n",
    "\n",
    "def clean_data(df, log):\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    log.append(\"Missing data percentage per column:\")\n",
    "    missing_pct = df.isna().mean() * 100\n",
    "    log.append(missing_pct.to_string())\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col != \"temperature_2m\" and df[col].isna().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            log.append(f\"Imputed missing values in numeric column '{col}' with median: {median_val}\")\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "            log.append(f\"Imputed missing values in categorical column '{col}' with mode: {mode_val}\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['period', 'date'] or (\"date\" in col.lower() or \"time\" in col.lower()):\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                if df[col].isna().sum() > 0 and df[col].notna().sum() > 0:\n",
    "                    mode_date = df[col].mode()[0]\n",
    "                    df[col] = df[col].fillna(mode_date)\n",
    "                    log.append(f\"Imputed missing values in date column '{col}' with mode: {mode_date}\")\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                log.append(f\"Converted column '{col}' to datetime.\")\n",
    "\n",
    "                prefix = \"extracted_\" + col\n",
    "                if df[col].notna().sum() > 0:\n",
    "                    df[prefix + \"_hour\"] = df[col].dt.hour\n",
    "                    df[prefix + \"_day\"] = df[col].dt.day\n",
    "                    df[prefix + \"_month\"] = df[col].dt.month\n",
    "                    df[prefix + \"_dayofweek\"] = df[col].dt.dayofweek\n",
    "                    for feat, rng in [(\"_hour\", (0,23)), (\"_day\", (1,28)), (\"_month\", (1,12)), (\"_dayofweek\", (0,6))]:\n",
    "                        feat_col = prefix + feat\n",
    "                        if df[feat_col].isna().sum() > 0 and df[feat_col].notna().sum() > 0:\n",
    "                            median_val = int(round(df[feat_col].median()))\n",
    "                            df[feat_col] = df[feat_col].fillna(median_val)\n",
    "                            log.append(f\"Imputed missing values in {feat_col} with median: {median_val}\")\n",
    "                    df[prefix + \"_is_weekend\"] = df[prefix + \"_dayofweek\"].apply(lambda x: x >= 5)\n",
    "                    df[prefix + \"_season\"] = df[prefix + \"_month\"].apply(get_season)\n",
    "                else:\n",
    "                    log.append(f\"Column '{col}' has no valid dates; no temporal features extracted.\")\n",
    "            except Exception as e:\n",
    "                log.append(f\"Error processing date column '{col}': {e}\")\n",
    "\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    duplicates_removed = initial_rows - df.shape[0]\n",
    "    if duplicates_removed > 0:\n",
    "        log.append(f\"Removed {duplicates_removed} duplicate rows.\")\n",
    "\n",
    "    constant_columns = [col for col in df.columns if df[col].nunique() <= 1 and col.lower() != \"value\"]\n",
    "    if constant_columns:\n",
    "        df.drop(columns=constant_columns, inplace=True)\n",
    "        log.append(f\"Dropped constant columns (excluding 'value'): {constant_columns}\")\n",
    "\n",
    "    outlier_summary = {}\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns and col != \"temperature_2m\":\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            before_rows = df.shape[0]\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "            after_rows = df.shape[0]\n",
    "            removed = before_rows - after_rows\n",
    "            if removed > 0:\n",
    "                outlier_summary[col] = f\"Removed {removed} outliers\"\n",
    "                log.append(f\"Removed {removed} outliers from column '{col}'.\")\n",
    "            else:\n",
    "                outlier_summary[col] = \"No outliers\"\n",
    "                log.append(f\"No outliers found in column '{col}'.\")\n",
    "\n",
    "    if \"temperature_2m\" in df.columns:\n",
    "        if df[\"temperature_2m\"].nunique() > 1:\n",
    "            Q1 = df[\"temperature_2m\"].quantile(0.25)\n",
    "            Q3 = df[\"temperature_2m\"].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            df[\"temperature_anomaly\"] = ~df[\"temperature_2m\"].between(lower_bound, upper_bound)\n",
    "            n_anomalies = df[\"temperature_anomaly\"].sum()\n",
    "            outlier_summary[\"temperature_2m\"] = f\"Marked {n_anomalies} anomalies\"\n",
    "            log.append(f\"Marked {n_anomalies} anomalies in 'temperature_2m'.\")\n",
    "            df = df.sort_values(by=\"temperature_anomaly\", ascending=True).reset_index(drop=True)\n",
    "        else:\n",
    "            outlier_summary[\"temperature_2m\"] = \"No outliers (constant)\"\n",
    "            log.append(\"Temperature column 'temperature_2m' is constant; no anomaly marking applied.\")\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            std_col = col + \"_std\"\n",
    "            df[std_col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "            log.append(f\"Created standardized feature '{std_col}'.\")\n",
    "\n",
    "    log.append(\"\\n***** Outlier Summary *****\")\n",
    "    for col, summary in outlier_summary.items():\n",
    "        log.append(f\"{col}: {summary}\")\n",
    "\n",
    "    return df, outlier_summary\n",
    "def cleaner():\n",
    "    log = []\n",
    "\n",
    "    input_file = merged_csv_output\n",
    "    output_file = os.path.join(output_dir, \"cleaned.csv\")\n",
    "\n",
    "    # Verify input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        st.write(f\"Input file does not exist: {input_file}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the input CSV\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except Exception as e:\n",
    "        st.write(f\"Error reading CSV: {e}\")\n",
    "        return\n",
    "    \n",
    "    log.append(f\"Original DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_df, outlier_summary = clean_data(df, log)\n",
    "    log.append(f\"Cleaned DataFrame shape: {cleaned_df.shape}\")\n",
    "    \n",
    "    # Save the cleaned DataFrame\n",
    "    try:\n",
    "        cleaned_df.to_csv(output_file, index=False)\n",
    "        log.append(f\"Cleaned CSV saved as {output_file}\")\n",
    "        st.write(f\"Cleaned CSV saved as {output_file}\")\n",
    "    except Exception as e:\n",
    "        log.append(f\"Error saving cleaned CSV: {e}\")\n",
    "        st.write(f\"Error saving cleaned CSV: {e}\")\n",
    "    \n",
    "    # Generate summary statistics and print log\n",
    "    try:\n",
    "        summary_stats = cleaned_df.describe(include='all')\n",
    "        st.write(\"Summary Statistics for Cleaned Data\")\n",
    "        st.write(summary_stats)\n",
    "        st.write(\"\\n***** Processing Log *****\")\n",
    "        st.write(pd.DataFrame(log))\n",
    "    except Exception as e:\n",
    "        st.write(f\"Error generating summary statistics: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ec94d4-1c94-4015-a1d4-24eedadc4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import streamlit as st\n",
    "from paths import *\n",
    "\n",
    "def statistical_summary(df, output_dir):\n",
    "    \"\"\"\n",
    "    Compute key statistical metrics for selected numerical features and save to \"Statistical_Summary.txt\".\n",
    "    \"\"\"\n",
    "    key_features = ['value', 'temperature_2m', 'extracted_period_hour',\n",
    "                    'extracted_period_day', 'extracted_period_month', 'extracted_period_dayofweek']\n",
    "    existing_features = [col for col in key_features if col in df.columns]\n",
    "    \n",
    "    summary = df[existing_features].describe()\n",
    "    extra_stats = pd.DataFrame({\n",
    "        'skewness': df[existing_features].skew(),\n",
    "        'kurtosis': df[existing_features].kurtosis()\n",
    "    })\n",
    "    summary = pd.concat([summary, extra_stats])\n",
    "    \n",
    "    st.write(summary)\n",
    "\n",
    "def time_series_analysis(df, output_dir):\n",
    "    \"\"\"\n",
    "    Plot electricity demand over time.\n",
    "    \"\"\"\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df_sorted = df.sort_values(by='date').dropna(subset=['date', 'value'])\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_sorted['date'], df_sorted['value'], label='Electricity Demand', color='blue')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Electricity Demand\")\n",
    "    plt.title(\"Electricity Demand Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    st.pyplot(plt)\n",
    "\n",
    "def univariate_analysis(df, output_dir):\n",
    "    \"\"\"\n",
    "    Generate histograms, boxplots, and density plots for key numerical features.\n",
    "    \"\"\"\n",
    "    key_features = [\n",
    "        \"extracted_period_hour\", \"extracted_period_day\", \"extracted_period_month\", \n",
    "        \"extracted_period_dayofweek\", \"temperature_2m\"\n",
    "    ]\n",
    "    df = df[key_features].dropna()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(key_features), 3, figsize=(15, 5*len(key_features)))\n",
    "    \n",
    "    for i, col in enumerate(key_features):\n",
    "        col_data = df[col]\n",
    "        \n",
    "        sns.histplot(col_data, bins=30, kde=True, ax=axes[i, 0], color='skyblue')\n",
    "        axes[i, 0].set_title(f\"Histogram of {col}\")\n",
    "        \n",
    "        sns.boxplot(x=col_data, ax=axes[i, 1], color='lightcoral')\n",
    "        axes[i, 1].set_title(f\"Boxplot of {col}\")\n",
    "        \n",
    "        sns.kdeplot(col_data, ax=axes[i, 2], color='purple')\n",
    "        axes[i, 2].set_title(f\"Density Plot of {col}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    st.pyplot(plt)\n",
    "\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"\n",
    "    Compute correlation matrix and visualize using a heatmap.\n",
    "    \"\"\"\n",
    "    key_features = ['value', 'temperature_2m', 'extracted_period_hour',\n",
    "                    'extracted_period_day', 'extracted_period_month', 'extracted_period_dayofweek']\n",
    "    df = df[key_features].dropna()\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    \n",
    "    st.pyplot(plt)\n",
    "    \n",
    "def advanced_time_series_techniques(df):\n",
    "    \"\"\"\n",
    "    Perform time series decomposition and ADF test.\n",
    "    \"\"\"\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df_sorted = df.sort_values(by='date').dropna(subset=['date', 'value'])\n",
    "    ts = df_sorted.set_index('date')['value']\n",
    "    \n",
    "    decomposition = seasonal_decompose(ts, model='additive', period=24)\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(12, 8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    adf_result = adfuller(ts.dropna())\n",
    "    adf_output = (f\"ADF Statistic: {adf_result[0]:.4f}\\n\"\n",
    "                    f\"p-value: {adf_result[1]:.4f}\\n\"\n",
    "                    f\"Critical Values: {adf_result[4]}\")\n",
    "    \n",
    "    st.write(adf_output)\n",
    "\n",
    "def run_eda(input_csv):\n",
    "    \"\"\"\n",
    "    Load dataset, run all EDA functions, and save outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "    st.write(f\"Loaded dataset with shape: {df.shape}\")\n",
    "    \n",
    "    statistical_summary(df, output_dir)\n",
    "    time_series_analysis(df, output_dir)\n",
    "    univariate_analysis(df, output_dir)\n",
    "    correlation_analysis(df)\n",
    "    advanced_time_series_techniques(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a266b0-71d3-43de-88ec-8e3fe9526aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from paths import *\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data[column] < lower_bound) | (data[column] > upper_bound), lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    return np.abs(zscore(data[column])) > threshold\n",
    "\n",
    "def plot_before_after(df_original, df_capped, column):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Before\n",
    "    axes[0].boxplot(df_original[column].dropna(), vert=True)\n",
    "    axes[0].set_title(f\"Before Capping - {column}\")\n",
    "    \n",
    "    # After\n",
    "    axes[1].boxplot(df_capped[column].dropna(), vert=True)\n",
    "    axes[1].set_title(f\"After Capping - {column}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "\n",
    "def handle_outliers(df, num_cols):\n",
    "    outlier_summary = []\n",
    "    total_outliers = 0\n",
    "\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            # IQR-based detection\n",
    "            iqr_mask, lower_bound, upper_bound = detect_outliers_iqr(df, col)\n",
    "            \n",
    "            # Z-score detection\n",
    "            z_mask = detect_outliers_zscore(df, col)\n",
    "            \n",
    "            # Combine both outlier masks (logical OR)\n",
    "            combined_mask = iqr_mask | z_mask\n",
    "            \n",
    "            # Count outliers before capping\n",
    "            outlier_count = combined_mask.sum()\n",
    "            total_outliers += outlier_count\n",
    "            \n",
    "            # Record summary\n",
    "            outlier_summary.append({\n",
    "                \"column\": col,\n",
    "                \"iqr_outliers\": iqr_mask.sum(),\n",
    "                \"zscore_outliers\": z_mask.sum(),\n",
    "                \"combined_outliers\": outlier_count,\n",
    "                \"lower_bound\": lower_bound,\n",
    "                \"upper_bound\": upper_bound\n",
    "            })\n",
    "            \n",
    "            # cap outliers\n",
    "            df.loc[df[col] < lower_bound, col] = lower_bound\n",
    "            df.loc[df[col] > upper_bound, col] = upper_bound\n",
    "\n",
    "    return df, outlier_summary, total_outliers\n",
    "\n",
    "def generate_report(outlier_summary, total_outliers):\n",
    "    st.write(\"Outlier Handling Report\")\n",
    "\n",
    "    st.write(\"Column-by-Column Summary:\")\n",
    "    for summary in outlier_summary:\n",
    "        st.write(\n",
    "            f\"- {summary['column']}:\\n\"\n",
    "            f\"   IQR outliers: {summary['iqr_outliers']}\\n\"\n",
    "            f\"   Z-score outliers: {summary['zscore_outliers']}\\n\"\n",
    "            f\"   Combined outliers: {summary['combined_outliers']}\\n\"\n",
    "            f\"   Capping range: [{summary['lower_bound']:.2f}, {summary['upper_bound']:.2f}]\\n\"\n",
    "        )\n",
    "\n",
    "    st.write(f\"Total Outliers (across all columns): {total_outliers}\\n\")\n",
    "\n",
    "    st.write(\"Technical Rationale:\")\n",
    "    st.write(\"- Outliers can distort mean/variance, affect model performance, and skew visualizations.\")\n",
    "    st.write(\"- By capping rather than removing, we retain data size while mitigating extreme skew but this cause data to be biased.\\n\")\n",
    "\n",
    "\n",
    "    st.write(\"Decision:\")\n",
    "    st.write(\"  We applied capping based on IQR boundaries.\\n\")\n",
    "def outlier():\n",
    "    num_cols = [\"value\", \"temperature_2m\", \"value_std\", \"temperature_2m_std\"]\n",
    "    \n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    df_original = df.copy()\n",
    "    \n",
    "    df, outlier_summary, total_outliers = handle_outliers(df, num_cols)\n",
    "    \n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            plot_before_after(df_original, df, col)\n",
    "    \n",
    "    generate_report(outlier_summary, total_outliers)\n",
    "    st.write(\"Outlier detection, capping, and reporting complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b52510-a538-42a9-8be6-080210a42ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import streamlit as st\n",
    "from paths import *  \n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test) \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, mse, rmse, r2\n",
    "\n",
    "def plot_actual_vs_predicted(y_test, y_pred):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, color='blue')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "    plt.xlabel(\"Actual Electricity Demand\")\n",
    "    plt.ylabel(\"Predicted Electricity Demand\")\n",
    "    plt.title(\"Actual vs. Predicted Electricity Demand\")\n",
    "    st.pyplot(plt)\n",
    "\n",
    "def residual_analysis(y_test, y_pred):\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Residual Analysis\")\n",
    "    st.pyplot(plt)\n",
    "\n",
    "def plot_model_performance(mse, rmse, r2):\n",
    "    metrics = [mse, rmse, r2]\n",
    "    metric_names = ['Mean Squared Error', 'Root Mean Squared Error', 'R² Score']\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=metric_names, y=metrics, palette='viridis')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Metrics')\n",
    "    plt.ylim(0, max(metrics) * 1.1)  \n",
    "    st.pyplot(plt)\n",
    "\n",
    "def regression():\n",
    "    features = [\n",
    "        \"extracted_period_hour\", \"extracted_period_day\", \"extracted_period_month\", \n",
    "        \"extracted_period_dayofweek\", \"temperature_2m\"\n",
    "    ]\n",
    "    target = \"value\"  \n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv(input_file)\n",
    "    df[features] = df[features].fillna(df[features].median())\n",
    "    df[target] = df[target].fillna(df[target].median())\n",
    "    x,y = df[features],df[target]\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred, mse, rmse, r2 = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    st.write(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    st.write(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    st.write(f\"R² Score: {r2}\")\n",
    "    st.write(\"Model R^2 Score:\", model.score(X_test, y_test))\n",
    "\n",
    "    # Plot results\n",
    "    plot_actual_vs_predicted(y_test, y_pred)\n",
    "    residual_analysis(y_test, y_pred)\n",
    "    plot_model_performance(mse, rmse, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f43df-c921-4da5-a114-068d349d0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 11:48:01.216 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Taqi\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "C:\\Users\\Taqi\\Desktop\\EDA-Pipeline\\processor.py:142: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n",
      "C:\\Users\\Taqi\\Desktop\\EDA-Pipeline\\processor.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"temperature_anomaly\"] = ~df[\"temperature_2m\"].between(lower_bound, upper_bound)\n",
      "2025-03-02 11:48:40.472 Serialization of dataframe to Arrow table was unsuccessful due to: (\"Could not convert Timestamp('2024-01-09 19:46:08.115450880+0000', tz='UTC') with type Timestamp: tried to convert to int64\", 'Conversion failed for column date with type object'). Applying automatic fixes for column types to make the dataframe Arrow-compatible.\n"
     ]
    }
   ],
   "source": [
    "from eda import run_eda\n",
    "from loader import merger\n",
    "from outlier import outlier\n",
    "from processor import cleaner\n",
    "from regression import regression\n",
    "from paths import input_file\n",
    "import streamlit as st\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    st.title(\"Electricity Demand Forecasting\")\n",
    "    st.header(\"Introduction\")\n",
    "    st.write(\"This application performs data preprocessing, exploratory data analysis, outlier detection, and regression analysis on electricity demand data.\")\n",
    "    st.header(\"Data Loading\")\n",
    "    merger()\n",
    "    st.header(\"Data Preprocessing\")\n",
    "    cleaner()\n",
    "    st.header(\"Exploratory Data Analysis\")\n",
    "    run_eda(input_file)\n",
    "    st.header(\"Outlier Detection\")\n",
    "    outlier()\n",
    "    st.header(\"Regression Analysis\")\n",
    "    regression()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c452c2-89b7-4c4e-905a-c3d54e4aad19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
