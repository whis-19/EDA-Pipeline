{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44020835-ea81-4dc7-bd62-4f92b9bbf8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "electricity_data = r\".\\raw\\electricity_raw_data\"\n",
    "weather_data = r\".\\raw\\weather_raw_data\"\n",
    "output_dir = r\".\\merged_output\"\n",
    "output_file = os.path.join(output_dir, \"merged.csv\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def load_electricity_data(folder_path):\n",
    "    electricity_data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(folder_path, file), 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)[\"response\"][\"data\"]\n",
    "                    electricity_data.extend(data)\n",
    "                except KeyError:\n",
    "                    print(f\"Skipping file {file} due to unexpected format\")\n",
    "    electricity_df = pd.DataFrame(electricity_data)\n",
    "    electricity_df[\"period\"] = pd.to_datetime(electricity_df[\"period\"], errors='coerce')\n",
    "    electricity_df[\"value\"] = electricity_df[\"value\"].astype(str).str.extract(r'([0-9]+\\.?[0-9]*)')[0]\n",
    "    electricity_df[\"value\"] = pd.to_numeric(electricity_df[\"value\"], errors='coerce')\n",
    "    electricity_df = electricity_df.rename(columns={\"period\": \"datetime\", \"value\": \"demand_mwh\"})\n",
    "    electricity_df[\"datetime\"] = pd.to_datetime(electricity_df[\"datetime\"], errors='coerce', utc=True)\n",
    "    return electricity_df.dropna()\n",
    "\n",
    "def load_weather_data(folder_path):\n",
    "    weather_data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder_path, file))\n",
    "            df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors='coerce', utc=True)\n",
    "            weather_data.append(df)\n",
    "    weather_df = pd.concat(weather_data, ignore_index=True)\n",
    "    weather_df = weather_df.rename(columns={\"date\": \"datetime\", \"temperature_2m\": \"temperature\"})\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"datetime\"], errors='coerce', utc=True)\n",
    "    return weather_df.dropna()\n",
    "\n",
    "def merge_data():\n",
    "    # Load data\n",
    "    electricity_df = load_electricity_data(electricity_data)\n",
    "    weather_df = load_weather_data(weather_data)\n",
    "    \n",
    "    # Merge data\n",
    "    data = pd.merge(electricity_df, weather_df, on=\"datetime\", how=\"inner\")\n",
    "    data[\"demand_mwh\"] = pd.to_numeric(data[\"demand_mwh\"], errors='coerce')\n",
    "    data.sort_values(by=\"datetime\", inplace=True)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save merged data\n",
    "    data.to_csv(output_file, index=False)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d6671-b8b5-4302-8219-b8d43b8721c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import streamlit as st\n",
    "\n",
    "def feature_engineering(data):\n",
    "    \n",
    "    # Feature Engineering\n",
    "    data[\"hour\"] = data[\"datetime\"].dt.hour\n",
    "    data[\"day\"] = data[\"datetime\"].dt.day\n",
    "    data[\"month\"] = data[\"datetime\"].dt.month\n",
    "    data['year'] = data[\"datetime\"].dt.year\n",
    "    data[\"day_of_week\"] = data[\"datetime\"].dt.dayofweek\n",
    "    data[\"is_weekend\"] = data[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    data[\"season\"] = data[\"month\"].apply(lambda x: 'Winter' if x in [12, 1, 2] else 'Spring' if x in [3, 4, 5] else 'Summer' if x in [6, 7, 8] else 'Fall')\n",
    "    data = pd.get_dummies(data, columns=['season'], drop_first=True)\n",
    "    data = pd.get_dummies(data, columns=['subba-name'], drop_first=True)\n",
    "    # Convert only the new dummy columns to int\n",
    "    dummy_cols = [col for col in data.columns if 'Province_' in col]\n",
    "    data[dummy_cols] = data[dummy_cols].astype(int)\n",
    "\n",
    "    return data\n",
    "\n",
    "def data_type_conversions(data):\n",
    "    # Ensure demand_mwh is numeric\n",
    "    data[\"demand_mwh\"] = pd.to_numeric(data[\"demand_mwh\"], errors='coerce')\n",
    "\n",
    "    # Sort data by datetime\n",
    "    data.sort_values(by=\"datetime\", inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def test_mcar(df):\n",
    "    df_miss = df.isnull().astype(int)\n",
    "    chi2, p, _, _ = chi2_contingency(df_miss.corr())\n",
    "    return p  # If p > 0.05, data is MCAR\n",
    "\n",
    "def data_cleaning_and_consistency(df):\n",
    "    # 1. Identify missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "    # 2. Display missing values summary\n",
    "    missing_summary = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "    st.write(\"Missing Values Summary:\")\n",
    "    st.write(missing_summary[missing_summary['Missing Values'] > 0])\n",
    "    \n",
    "    p_value = test_mcar(df)\n",
    "    if p_value > 0.05:\n",
    "        st.write(\"Missing data is likely MCAR (Missing Completely at Random)\")\n",
    "    else:\n",
    "        st.write(\"Missing data is likely MAR (Missing at Random) or MNAR (Not Missing at Random)\")\n",
    "\n",
    "    # 5. Handling missing data\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype == 'object':  # Categorical columns\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)  # Impute with mode\n",
    "            else:  # Numerical columns\n",
    "                df[col].fillna(df[col].median(), inplace=True)  # Impute with median\n",
    "\n",
    "    st.write(\"Missing values handled using mode (categorical) or median (numerical)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_duplicates_and_anomalies(df):\n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Identify numerical columns\n",
    "    num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Detect outliers using IQR method\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Mark outliers\n",
    "        outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        st.write(f\"Outliers detected in {col}: {outliers.sum()} rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Identify numerical columns for normalization/standardization\n",
    "    num_cols = data.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Choose either StandardScaler (Z-score normalization) or MinMaxScaler (scales to [0,1])\n",
    "    scaler = StandardScaler()\n",
    "    data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "    return data\n",
    "\n",
    "# def normalize_data(data):\n",
    "#     # Normalize only the 'value' column\n",
    "#     if 'value' in data.columns:\n",
    "#         scaler = StandardScaler()\n",
    "#         data['value'] = scaler.fit_transform(data[['value']])\n",
    "#     return data\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    df = data_type_conversions(df)\n",
    "    df= data_cleaning_and_consistency(df)\n",
    "    df = handle_duplicates_and_anomalies(df)\n",
    "    df = feature_engineering(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f1707-0f2c-49c9-96c8-b97c0778432b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 34) (1691065487.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 34)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3f272-a133-49c6-82e6-6c13399eca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def statistical_summary(df):\n",
    "    \"\"\"\n",
    "    Computes statistical metrics for each numerical variable in the dataframe and returns\n",
    "    a prettified DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with numerical variables\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the statistical summaries\n",
    "    \"\"\"\n",
    "    # Select numerical columns\n",
    "    num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    if len(num_cols) == 0:\n",
    "        return \"No numerical columns found in the DataFrame.\"\n",
    "\n",
    "    # Dictionary to store summaries for each variable\n",
    "    summary_dict = {}\n",
    "    for col in num_cols:\n",
    "            summary_dict[col] = {\n",
    "                \"Mean\": df[col].mean(),\n",
    "                \"Median\": df[col].median(),\n",
    "                \"Standard Deviation\": df[col].std(),\n",
    "                \"Variance\": df[col].var(),\n",
    "                \"Skewness\": df[col].skew(),\n",
    "                \"Kurtosis\": stats.kurtosis(df[col], fisher=True),\n",
    "                \"Min\": df[col].min(),\n",
    "                \"Max\": df[col].max(),\n",
    "                \"25th Percentile\": df[col].quantile(0.25),\n",
    "                \"50th Percentile (Median)\": df[col].quantile(0.50),\n",
    "                \"75th Percentile\": df[col].quantile(0.75)\n",
    "            }\n",
    "\n",
    "        # Convert dictionary to DataFrame for pretty display\n",
    "    summary_df = pd.DataFrame(summary_dict).T\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def plot_time_series(df, datetime_col=\"datetime\", demand_col=\"demand_mwh\", sample_rate=1000):\n",
    "    \"\"\"\n",
    "    Plots a clean time series line chart without unwanted fill effects.\n",
    "\n",
    "    :param df: Pandas DataFrame containing time series data\n",
    "    :param datetime_col: Column name representing timestamps\n",
    "    :param demand_col: Column name representing electricity demand\n",
    "    :param sample_rate: Interval to downsample data for better visualization\n",
    "    \"\"\"\n",
    "     # Convert datetime column to proper format\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "\n",
    "    # Sort data by time\n",
    "    df = df.sort_values(by=datetime_col)\n",
    "\n",
    "    # Downsample data (optional but improves clarity)\n",
    "    # df_sampled = df.iloc[::sample_rate]  # Pick every nth row\n",
    "    df_sampled = df\n",
    "\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot as a thin line (no fill)\n",
    "    plt.plot(df_sampled[datetime_col], df_sampled[demand_col], color='blue', linewidth=1, linestyle='-')\n",
    "\n",
    "    # Titles and labels\n",
    "    plt.title(\"Electricity Demand Over Time\", fontsize=14)\n",
    "    plt.xlabel(\"Time\", fontsize=12)\n",
    "    plt.ylabel(\"Electricity Demand\", fontsize=12)\n",
    "\n",
    "    # Rotate x-axis labels for better visibility\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Grid for better visualization\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "def univariate_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform univariate analysis: Histogram, Boxplot, Density plot.\n",
    "    \n",
    "    :param df: Pandas DataFrame containing the data\n",
    "    :param column: Column name (numerical feature) to analyze\n",
    "    \"\"\"\n",
    "      # Convert datetime column to actual datetime format\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "    # Selecting only numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    # Iterate over each numerical column\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(18, 5))\n",
    "\n",
    "        # Histogram\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.histplot(df[col], bins=30, kde=True, color='blue')\n",
    "        plt.title(f\"Histogram of {col}\")\n",
    "\n",
    "        # Boxplot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.boxplot(y=df[col], color='green')\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "\n",
    "        # Density Plot\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.kdeplot(df[col], fill=True, color='red')\n",
    "        plt.title(f\"Density Plot of {col}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Statistical Summary\n",
    "        stats = df[col].describe()\n",
    "        skewness = df[col].skew()\n",
    "        kurtosis = df[col].kurtosis()\n",
    "\n",
    "        print(f\"\\n🔹 Statistical Summary for {col}:\\n\")\n",
    "        print(stats)\n",
    "        print(f\"Skewness: {skewness}\")\n",
    "        print(f\"Kurtosis: {kurtosis}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def correlation_analysis(df, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Computes and visualizes the correlation matrix for numerical features.\n",
    "    Identifies multicollinearity issues by flagging highly correlated features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "    threshold (float): Correlation threshold for identifying multicollinearity (default = 0.75)\n",
    "\n",
    "    Returns:\n",
    "    high_corr_pairs (list): List of highly correlated feature pairs\n",
    "    \"\"\"\n",
    "    # Select numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "    # Identifying Multicollinearity (correlation > threshold)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "  # Print highly correlated features\n",
    "    if high_corr_pairs:\n",
    "        print(\"\\n🔹 Highly Correlated Feature Pairs (|correlation| > {}):\".format(threshold))\n",
    "        for feature1, feature2, correlation in high_corr_pairs:\n",
    "            print(f\"{feature1} ↔ {feature2} | Correlation: {correlation:.2f}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No strong multicollinearity detected (correlation > {}).\".format(threshold))\n",
    "    \n",
    "    return high_corr_pairs\n",
    "\n",
    "def time_series_analysis(df, date_col, target_col, period=24):\n",
    "    \"\"\"\n",
    "    Performs time series decomposition and stationarity testing (ADF Test).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with time series data\n",
    "    date_col (str): Column name containing datetime information\n",
    "    target_col (str): Column name for the time series variable (e.g., demand)\n",
    "    period (int): Seasonal period for decomposition (default = 24 for hourly data)\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Convert date column to datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df.set_index(date_col, inplace=True)\n",
    "\n",
    "    # Time Series Decomposition\n",
    "    decomposition = sm.tsa.seasonal_decompose(df[target_col], model='additive', period=period)\n",
    "\n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 8))\n",
    "    decomposition.observed.plot(ax=axes[0], title=\"Observed\")\n",
    "    decomposition.trend.plot(ax=axes[1], title=\"Trend\")\n",
    "    decomposition.seasonal.plot(ax=axes[2], title=\"Seasonality\")\n",
    "    decomposition.resid.plot(ax=axes[3], title=\"Residuals\", linestyle='dashed')\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(\"Date\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Stationarity Test: Augmented Dickey-Fuller Test\n",
    "    print(\"\\n📉 Augmented Dickey-Fuller Test Results:\")\n",
    "    adf_test = adfuller(df[target_col].dropna())\n",
    "    results = pd.Series(adf_test[:4], index=['Test Statistic', 'p-value', '# Lags Used', '# Observations Used'])\n",
    "    for key, value in adf_test[4].items():\n",
    "        results[f'Critical Value ({key})'] = value\n",
    "\n",
    "    print(results)\n",
    "\n",
    "    # Interpretation\n",
    "    if adf_test[1] < 0.05:\n",
    "        print(\"\\n✅ The time series is stationary (p-value < 0.05).\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ The time series is non-stationary (p-value >= 0.05). Consider differencing or detrending.\")\n",
    "\n",
    "    # Reset index after analysis\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "def check_demand_in_each_city(data):\n",
    "    # Extract province columns (all columns starting with \"Province_\")\n",
    "    province_columns = [col for col in data.columns if col.startswith(\"Province_\")]\n",
    "    \n",
    "    # Multiply each one-hot encoded province column by the demand to distribute demand correctly\n",
    "    province_demand = data[province_columns].multiply(data[\"demand_mwh\"], axis=0).sum()\n",
    "\n",
    "    # Sort values\n",
    "    province_demand = province_demand.sort_values()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(y=province_demand.index.str.replace(\"Province_\", \"\"), x=province_demand.values, palette=\"coolwarm\", orient='h')\n",
    "    plt.ylabel(\"Province\")\n",
    "    plt.xlabel(\"Total Electricity Demand (MWh)\")\n",
    "    plt.title(\"Electricity Demand by Province\")\n",
    "    plt.show()\n",
    "\n",
    "    return province_demand\n",
    "\n",
    "def perform_eda(df):\n",
    "\n",
    "    check_demand_in_each_city(df)\n",
    "\n",
    "    statistical_summary_result = statistical_summary(df)\n",
    "    print(statistical_summary_result)\n",
    "\n",
    "    plot_time_series(df)\n",
    "\n",
    "    univariate_analysis(df)\n",
    "\n",
    "    high_corr_pairs = correlation_analysis(df)\n",
    "\n",
    "    time_series_analysis(df, \"datetime\", \"demand_mwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d848c08d-8a9c-46f1-a5db-9619f89828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    return outliers\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    z_scores = np.abs(zscore(data[column]))\n",
    "    return z_scores > threshold\n",
    "\n",
    "def detect_and_handle_outliers(df, column, method=\"remove\"):\n",
    "    \"\"\"\n",
    "    Detects and handles outliers in the specified column of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset containing the column.\n",
    "        column (str): The column name to analyze.\n",
    "        method (str): Strategy to handle outliers - \"remove\", \"cap\", or \"transform\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset after applying the outlier handling strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle Outliers Based on the Selected Method\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    if method == \"remove\":\n",
    "        iqr_outliers = detect_outliers_iqr(df, column)\n",
    "        df_cleaned = df_cleaned[~iqr_outliers]  # Remove outliers\n",
    "    elif method == \"cap\":\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_cleaned[column] = np.clip(df_cleaned[column], lower_bound, upper_bound)  # Cap values\n",
    "    elif method == \"transform\":\n",
    "        df_cleaned[column] = np.log1p(df_cleaned[column])  # Log transformation\n",
    "\n",
    "    # Plot Before and After\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    axs[0].hist(df[column], bins=50, color=\"blue\", alpha=0.7)\n",
    "    axs[0].set_title(f\"Original {column} Distribution\")\n",
    "\n",
    "    axs[1].hist(df_cleaned[column], bins=50, color=\"red\", alpha=0.7)\n",
    "    axs[1].set_title(f\"After Applying {method.capitalize()} Method\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def outliers(df):\n",
    "\n",
    "    # Apply to demand_mwh column\n",
    "    iqr_outliers = detect_outliers_iqr(df, \"demand_mwh\")\n",
    "    print(\"IQR Outliers (demand_mwh):\", iqr_outliers.sum())\n",
    "    zscore_outliers = detect_outliers_zscore(df, \"demand_mwh\")\n",
    "    print(\"Z-score Outliers (demand_mwh):\", zscore_outliers.sum())\n",
    "    \n",
    "    # Apply to temperature column\n",
    "    iqr_outliers = detect_outliers_iqr(df, \"temperature\")\n",
    "    print(\"IQR Outliers (temperature):\", iqr_outliers.sum())\n",
    "    zscore_outliers = detect_outliers_zscore(df, \"temperature\")\n",
    "    print(\"Z-score Outliers (temperature):\", zscore_outliers.sum())\n",
    "\n",
    "    # Run Outlier Detection & Handling\n",
    "    cleaned_data = detect_and_handle_outliers(df, \"demand_mwh\", method=\"transform\")\n",
    "    cleaned_data = detect_and_handle_outliers(cleaned_data, \"temperature\", method=\"remove\")\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143dcb4-a9ba-4884-8fa5-c0fa395bf313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def regression_model(df, target, time_column, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a regression model to predict electricity demand.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The preprocessed dataset.\n",
    "        target (str): The column name of the target variable (e.g., electricity demand).\n",
    "        time_column (str): The column representing timestamps.\n",
    "        test_size (float): Proportion of data used for testing (default = 0.2).\n",
    "\n",
    "    Returns:\n",
    "        model (LinearRegression): Trained regression model.\n",
    "        predictions (np.array): Predictions on test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define Features (Excluding Non-Numeric Columns)\n",
    "    feature_cols = [\"hour\", \"day\", \"month\", \"year\", \"day_of_week\", \"is_weekend\", \"season_Spring\", \"season_Summer\", \"season_Winter\"]\n",
    "    if \"temperature\" in df.columns:\n",
    "        feature_cols.append(\"temperature\")  # Include temperature if available\n",
    "\n",
    "    province_columns = [col for col in df.columns if col.startswith(\"Province_\")]\n",
    "    feature_cols.extend(province_columns)\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df[target]\n",
    "\n",
    "    # Split into Training & Testing Sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Train Linear Regression Model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate Model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Model Performance Metrics:\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\")\n",
    "    print(f\"Model Score: {model.score(X_test, y_test):.2f}\")\n",
    "\n",
    "    # Plot Actual vs Predicted\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, color=\"blue\")\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], '--r', linewidth=2)  # Perfect fit line\n",
    "    plt.xlabel(\"Actual Demand\")\n",
    "    plt.ylabel(\"Predicted Demand\")\n",
    "    plt.title(\"Actual vs Predicted Electricity Demand\")\n",
    "    plt.show()\n",
    "\n",
    "    # Residual Plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, color=\"purple\")\n",
    "    plt.axvline(0, color='red', linestyle='dashed', linewidth=2)\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.title(\"Residual Analysis\")\n",
    "    plt.show()\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494928f1-fdda-4159-860f-ab2e95beeffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploratory Data Analysis and Regression Model\n",
      "Data Loading\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Data Loading\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Loading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m merge_data()\n\u001b[0;32m     13\u001b[0m raw_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_merged_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw data loaded and saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_merged_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\EDA-Pipeline\\loader.py:49\u001b[0m, in \u001b[0;36mmerge_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_data\u001b[39m():\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     electricity_df \u001b[38;5;241m=\u001b[39m load_electricity_data(electricity_data)\n\u001b[1;32m---> 49\u001b[0m     weather_df \u001b[38;5;241m=\u001b[39m load_weather_data(weather_data)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Merge data\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(electricity_df, weather_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\EDA-Pipeline\\loader.py:37\u001b[0m, in \u001b[0;36mload_weather_data\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file))\n\u001b[0;32m     38\u001b[0m         df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mstrip(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m         df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "from eda import perform_eda\n",
    "from loader import merge_data\n",
    "from processor import process_data, normalize_data\n",
    "from outlier import outliers\n",
    "from regression import regression_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Exploratory Data Analysis and Regression Model\")\n",
    "\n",
    "    # Data Loading\n",
    "    print(\"Data Loading\")\n",
    "    raw_df = merge_data()\n",
    "    raw_df.to_csv(\"raw_merged_data.csv\", index=False)\n",
    "    print(\"Raw data loaded and saved as 'raw_merged_data.csv'.\")\n",
    "\n",
    "    # Data Processing\n",
    "    print(\"Data Processing\")\n",
    "    processed_df = process_data(raw_df)\n",
    "    processed_df.to_csv(\"processed_data.csv\", index=False)\n",
    "    print(\"Processed data saved as 'processed_data.csv'.\")\n",
    "\n",
    "    # EDA\n",
    "    print(\"Exploratory Data Analysis (EDA)\")\n",
    "    perform_eda(processed_df)\n",
    "    print(\"EDA performed on processed data.\")\n",
    "\n",
    "    # Outlier Detection\n",
    "    print(\"Outlier Detection\")\n",
    "    cleaned_df = outliers(processed_df)\n",
    "    cleaned_df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "    print(\"Outliers detected and cleaned data saved as 'cleaned_data.csv'.\")\n",
    "\n",
    "    # Normalization\n",
    "    print(\"Normalization\")\n",
    "    normalized_df = normalize_data(cleaned_df)\n",
    "    normalized_df.to_csv(\"normalized_data.csv\", index=False)\n",
    "    print(\"Normalized data saved as 'normalized_data.csv'.\")\n",
    "\n",
    "    # Regression Model\n",
    "    print(\"Regression Model\")\n",
    "    model, predictions = regression_model(normalized_df, target=\"demand_mwh\", time_column=\"datetime\")\n",
    "    print(\"Regression model built and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3914ff77-1435-4b77-87b0-e125f3bbf2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalized_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Regression Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegression Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model, predictions \u001b[38;5;241m=\u001b[39m regression_model(normalized_df, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemand_mwh\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegression model built and evaluated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_df' is not defined"
     ]
    }
   ],
   "source": [
    "    # Regression Model\n",
    "    print(\"Regression Model\")\n",
    "    model, predictions = regression_model(normalized_df, target=\"demand_mwh\", time_column=\"datetime\")\n",
    "    print(\"Regression model built and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72c58d-1b73-4598-8e70-5938cf1edcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
